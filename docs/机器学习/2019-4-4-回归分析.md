

实习的项目是做回归分析的，于是就有了以下的学习之路了,本科学的数学，所以也有一定的了解，但是很深入的东西就不是很了解了。之前机器学习入门的时候看过的书，一般都是先讲这些基本的算法，现在也忘得差不多了，哎，这些东西还是得多用才能理解得更深呀！

## 什么是回归分析？

### 回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。回归分析是建模和分析数据的重要工具。使用回归分析的好处包括：

1. 它可以表明自变量和因变量之间的显著关系
2. 可表明多个自变量对一个因变量的影响强度
3. 回归分析也允许我们去比较那些不同尺度的变量之间的相互影响，有利于帮助研究人员、数据分析人员以及数据科学家排除并估计出一组最佳的变量，用来构建预测模型。

### 两连续变量的线性回归模型的适用条件：

1. 线性趋势：自变量与因变量的关系是线性的，可通过散点图来判断;
2. 独立性：因变量y的取值相互独立的，之间没有联系。就是要求残差间相互独立，不存在自相关性，否则应采用自回归模型;
3. 正态性：因变量y均服从正态分布，即要求残差服从正态分布;
4. 方差齐性：自变量的任何线性组合中，因变量的方差均相同。即残差的方差要齐性。

***回归关系并不一定代表两者有因果关系。***

## 回归分析方法

### Linear Regression线性回归

它是最为人熟知的建模技术之一。线性回归通常是人们在学习预测模型时首选的技术之一。在这种技术中，因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。
线性回归使用最佳的拟合直线（也就是回归线）在因变量$Y$和一个或多个自变量$X$之间建立一种关系。
用一个方程式来表示它，即：
$$Y=a\times X+b+e$$
其中$b$表示截距，$a$表示直线的斜率，$e$是误差项。这个方程可以根据给定的预测变量$x$来预测目标变量的值。

一元线性回归和多元线性回归的区别在于，多元线性回归有＞1个自变量，而一元线性回归通常只有1个自变量。

那么如何获得最佳拟合线,即如何求得最佳的$a和b的值$？这个问题可以使用最小二乘法轻松地完成。最小二乘法也是用于拟合回归线最常用的方法。对于观测数据，它通过最小化每个数据点到线的垂直偏差平方和来计算最佳拟合线。因为在相加时，偏差先平方，所以正值和负值没有抵消。但用最小二乘法有一些限制：

1. 自变量与因变量之间必须有线性关系。
2. 多元回归存在多重共线性，自相关性和异方差性。
3. 线性回归对异常值非常敏感。它会严重影响回归线，最终影响预测值。
4. 多重共线性会增加系数估计值的方差，使得在模型轻微变化下，估计非常敏感，结果就是系数估计值不稳定。
5. 在多个自变量的情况下，我们可以使用向前选择法，向后剔除法和逐步筛选法来选择最重要的自变量。

### Logistic Regression逻辑回归
逻辑回归是用来计算“事件成立”和“事件不成立”的概率。当因变量的类型属于二元（1/0，真/假，是/否）变量时，我们就应该使用逻辑回归。这里，Y的值从0到1，它可以用下方程表示：
$$odds=\frac{p}{1-p}\\
ln(odds)=ln\frac{p}{1－p}
$$

$p$表述具有某个特征的概率。而为什么要在公式中使用对数log呢？
因为在这里我们使用的是的二项分布（因变量），我们需要选择一个对于这个分布最佳的连结函数。它就是Logit函数。在上述方程中，通过观测样本的极大似然估计值来选择参数，而不是最小化平方和误差（如在普通回归使用的）。

#### 要点：

1. 它广泛地用于分类问题。
2. 逻辑回归不要求自变量和因变量是线性关系。它可以处理各种类型的关系，因为它对预测的相对风险指数OR使用了一个非线性的log转换。
3. 为了避免过拟合和欠拟合，我们应该包括所有重要的变量。有一个很好的方法来确保这种情况，就是使用逐步筛选方法来估计逻辑回归。
4. 它需要大的样本量，因为在样本数量较少的情况下，极大似然估计的效果比普通的最小二乘法差。
5. 自变量不应该相互关联的，即不具有多重共线性。然而，在分析和建模中，我们可以选择包含分类变量相互作用的影响。
6. 如果因变量的值是定序变量，则称它为序逻辑回归。
7. 如果因变量是多类的话，则称它为多元逻辑回归。

### Polynomial Regression多项式回归

对于一个回归方程，如果自变量的指数大于1，那么它就是多项式回归方程。如下方程所示：
$$y=a \times x^2+b$$
在这种回归技术中，最佳拟合线不是直线。而是一个用于拟合数据点的曲线。虽然会有一个诱导可以拟合一个高次多项式并得到较低的错误，但这可能会导致过拟合。你需要经常画出关系图来查看拟合情况，并且专注于保证拟合合理，既没有过拟合又没有欠拟合。明显地向两端寻找曲线点，看看这些形状和趋势是否有意义。更高次的多项式最后可能产生怪异的推断结果。

### Stepwise Regression逐步回归

在处理多个自变量时，我们可以使用这种形式的回归。在这种技术中，自变量的选择是在一个自动的过程中完成的，其中包括非人为操作。它是通过观察统计的值，如R-square，t-stats和AIC指标，来识别重要的变量。逐步回归通过同时添加/删除基于指定标准的协变量来拟合模型。这种建模技术的目的是使用最少的预测变量数来最大化预测能力。这也是处理高维数据集的方法之一。以下列出了一些最常用的逐步回归方法：

1. 标准逐步回归法做两件事情。即增加和删除每个步骤所需的预测。
2. 向前选择法从模型中最显著的预测开始，然后为每一步添加变量。
3. 向后剔除法与模型的所有预测同时开始，然后在每一步消除最小显着性的变量。

### Ridge Regression岭回归

岭回归分析是一种用于存在多重共线性（自变量高度相关）数据的技术。在多重共线性情况下，尽管最小二乘法（OLS）对每个变量很公平，但它们的差异很大，使得观测值偏移并远离真实值。岭回归通过给回归估计上增加一个偏差度，来降低标准误差。

#### 要点：

1. 除常数项以外，这种回归的假设与最小二乘回归类似；
2. 它收缩了相关系数的值，但没有达到零，这表明它没有特征选择功能；
3. 这是一个正则化方法，并且使用的是L2正则化。

### Lasso Regression套索回归

它类似于岭回归，Lasso（Least Absolute Shrinkage and Selection Operator）也会惩罚回归系数的绝对值大小。此外，它能够减少变化程度并提高线性回归模型的精度。Lasso回归与Ridge回归有一点不同，它使用的惩罚函数是绝对值，而不是平方。这导致惩罚（或等于约束估计的绝对值之和）值使一些参数估计结果等于零。使用惩罚值越大，进一步估计会使得缩小值趋近于零。这将导致我们要从给定的n个变量中选择变量。

#### 要点：

1. 除常数项以外，这种回归的假设与最小二乘回归类似；
2. 它收缩系数接近零（等于零），这确实有助于特征选择；
3. 这是一个正则化方法，使用的是L1正则化。

### ElasticNet回归

ElasticNet回归是Lasso和Ridge回归技术的混合体。它使用L1来训练并且用L2优先作为正则化矩阵。当有多个相关的特征时，ElasticNet是很有用的。Lasso会随机挑选他们其中的一个，而ElasticNet则会选择两个。Lasso和Ridge之间的实际的优点是，它允许ElasticNet继承循环状态下Ridge的一些稳定性。

#### 要点：

1. 在高度相关变量的情况下，它会产生群体效应；
2. 选择变量的数目没有限制；
3. 它可以承受双重收缩。

### Bayesian回归

在贝叶斯观点中，我们使用概率分布而不是点估计来进行线性回归。 y不被估计为单个值，而是被假定为从正态分布中抽取。 贝叶斯线性回归模型是：
$$
y \sim N\left(\beta^{T} X, \sigma^{2} I\right)
$$
输出y由一个以均值和方差为特征的正态（高斯）分布产生。 线性回归的均值是权重矩阵乘以预测矩阵的转置。 方差是标准差σ的平方（乘以恒等矩阵，因为这是模型的多维表达式）。
$$
P(\beta | y, X)=\frac{P(y | \beta, X) * P(\beta | X)}{P(y | X)}
$$
贝叶斯线性回归的目的不是找到模型参数的单一“最佳”值，而是确定模型参数的后验分布。 不仅是由概率分布产生的响应，而且假定模型参数也来自分布。 模型参数的后验概率取决于训练输入和输出：
$$
Posterior=\frac{\text {Likelihood} * \text { Prior }}{\text { Normalization }}
$$
这里，$P(\beta | y, X)$是给定输入和输出的模型参数的后验概率分布。 这等于输出$P(y | \beta, X)$乘以给定输入$P(\beta | X)$的β的先验概率并除以归一化常数的可能性。 这是贝叶斯定理的一个简单表达式，贝叶斯定理是贝叶斯推理的基本基础。与OLS相比，模型参数有一个后验分布，它与数据乘以参数先验概率的可能性成正比。在这里我们可以观察到贝叶斯线性回归的两个主要好处。

1. 先验：如果我们有领域知识，或者猜测模型参数应该是什么，那么我们可以将它们包括在我们的模型中，这与频率方法不同，后者假设所有参数都来自数据。如果我们没有提前做出任何估计，那么我们可以使用非信息性的先验来确定正态分布等参数。
2. 后验：执行贝叶斯线性回归的结果是基于数据和先验的可能模型参数的分布。这使我们能够量化我们对模型的不确定性：如果数据少，后验分布将更加分散。

随着数据点数量的增加，可能性会冲刷先验，并且在无限数据的情况下，参数的输出会收敛到从OLS获得的值。
作为分布的模型参数的表达形式包含了贝叶斯的世界观：我们从最初的估计开始，即先验，并且随着我们收集更多的证据，我们的模型变得不那么错了。贝叶斯推理是我们直觉的自然延伸。通常，我们有一个最初的假设，当我们收集支持或反驳我们想法的数据时，我们改变了我们的世界模型（理想情况下这是我们的理由）！

### Robust回归。

稳健回归（robust regression）是统计学稳健估计中的一种方法，其主要思路是将对异常值十分敏感的经典最小二乘回归中的目标函数进行修改。经典最小二乘回归以使误差平方和达到最小为其目标函数。因为方差为一不稳健统计量，故最小二乘回归是一种不稳健的方法。不同的目标函数定义了不同的稳健回归方法。常见的稳健回归方法有：最小中位平方(least median square；LMS)法、M估计法等。

### 随机森林

随机森林算法几乎不需要输入的准备。它们不需要测算就能够处理二分特征、分类特征、数值特征的数据。随机森林算法能完成隐含特征的选择，并且提供一个很好的特征重要度的选择指标。随机森林算法训练速度快。性能优化过程刚好又提高了模型的准确性，这种精彩表现并不常有，反之亦然。这种旨在多样化子树的子设定随机特征，同时也是一种突出的性能优化！调低给定任意节点的特征划分，能让你简单的处理带有上千属性的数据集。随机森林算法对指定使用的超参数（hyper-parameters ）并不十分敏感。为了要得到一个合适的模型，它们不需要做很多调整。只需使用大量的树，模型就不会产生很多偏差。大多数的随机森林算法的实现方法的参数设置初始值也都是合理的。随机森林算法可以应用于很多类别的模型任务。它们可以很好的处理回归问题，也能对分类问题应付自如（甚至可以产生合适的标准概率值）。作为一个可以高度并行化的算法，随机在大数据时候大有可为。这里也对常规的随机森林算法的优缺点做一个总结。

#### 优点

1. 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。
2. 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
3. 在训练后，可以给出各个特征对于输出的重要性
4. 由于采用了随机采样，训练出的模型的方差小，泛化能力强。
5. 相对于Boosting系列的Adaboost和GBDT， 随机森林实现比较简单。
6. 对部分特征缺失不敏感。

#### 缺点

1. 在某些噪音比较大的样本集上，随机森林模型容易陷入过拟合。
2. 取值划分比较多的特征容易对随机森林的决策产生更大的影响，从而影响拟合的模型的效果。

## 模型评估

模型估计出来后，我们要回答的问题是：

### 我们的模型拟合程度如何？或者说，这个模型对因变量的解释力如何？

SSA代表由自变量x引起的y的离差平方和，即回归平方和，代表回归模型的解释力
SSE代表由随机因素引起的y的离差平方和，即剩余平方和，代表回归模型未能解释的部分
SST为总的离差平方和，即我们仅凭y的平均值去估计y时所产生的误差
用模型能够解释的变异除以总的变异就是模型的拟合程度：
$$R^2=\frac{SSA}{SST}=1-SSE$$

$R^2$也被称为决定系数或判定系数

### 整个模型是否能显著预测因变量的变化？（F检验）

假设y与x的线性关系不明显，那么SSA相对SSE占有较大的比例的概率则越小。 换句话说，在y与x无线性关系的前提下，SSA相对SSE的占比越高的概率是越小的，这会呈现一定的概率分布。 统计学家告诉我们它满足F分布，如果SSA相对SSE占比较大的情况出现了，比如根据F分布，这个值出现的概率小于5%。那么，我们最好是拒绝y与x线性关系不显著的原始假设，认为二者存在显著的线性关系较为合适。

### 每个自变量是否能显著预测因变量的变化？（T检验）

回归系数的显著性检验是围绕回归系数的抽样分布（T分布）来进行的，推断过程类似于整个模型的检验过程。实际上，对于只有一个自变量的一元线性模型，模型的显著性检验和回归系数的检验是一致的，但对于多元线性模型来说，不适用。

## 回归模型的选择
当你只知道一个或两个技术时，往往很简单。然而，在我们的处理中，可选择的越多，选择正确的一个就越难。类似的情况下也发生在回归模型中。在多类回归模型中，基于自变量和因变量的类型、数据的维数以及数据的其它基本特征的情况下，选择最合适的技术非常重要。

### 选择正确的回归模型的关键因素：

1. 数据探索是构建预测模型的必然组成部分。在选择合适的模型时，比如识别变量的关系和影响时，它应该首选。
2. 比较适合于不同模型的优点，我们可以分析不同的指标参数，如统计意义的参数，R-square，Adjusted R-square，AIC，BIC以及误差项，另一个是Mallows'Cp准则。这个主要是通过将模型与所有可能的子模型进行对比（或谨慎选择他们），检查在你的模型中可能出现的偏差。
3. 交叉验证是评估预测模型最好的方法。将数据集分成两份（一份做训练，一份做验证），使用观测值和预测值之间的一个简单均方差来衡量预测精度。
4. 如果数据集是多个混合变量，那么就不应该选择自动模型选择方法，因为你应该不想在同一时间把所有变量放在同一个模型中。
5. 它也将取决于你的目的。可能会出现这样的情况，一个不太强大的模型与具有高度统计学意义的模型相比，更易于实现。
6. 回归正则化方法（Lasso，Ridge和ElasticNet）在高维和数据集变量之间多重共线性情况下运行良好。

## 回归分析的工具

### 回归分析软件：SPSS，matlab，python，R语言

### 回归分析包：sklearn、statsmodels
