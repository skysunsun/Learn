

## [在网上找了一份sklearn关于回归分析的代码](https://blog.csdn.net/Yeoman92/article/details/75051848)

```python
####3.1决策树回归####
from sklearn import tree
model_DecisionTreeRegressor = tree.DecisionTreeRegressor()
####3.2线性回归####
from sklearn import linear_model
model_LinearRegression = linear_model.LinearRegression()
####3.3SVM回归####
from sklearn import svm
model_SVR = svm.SVR()
####3.4KNN回归####
from sklearn import neighbors
model_KNeighborsRegressor = neighbors.KNeighborsRegressor()
####3.5随机森林回归####
from sklearn import ensemble
model_RandomForestRegressor = ensemble.RandomForestRegressor(n_estimators=20)#这里使用20个决策树
####3.6Adaboost回归####
from sklearn import ensemble
model_AdaBoostRegressor = ensemble.AdaBoostRegressor(n_estimators=50)#这里使用50个决策树
####3.7GBRT回归####
from sklearn import ensemble
model_GradientBoostingRegressor = ensemble.GradientBoostingRegressor(n_estimators=100)#这里使用100个决策树
####3.8Bagging回归####
from sklearn.ensemble import BaggingRegressor
model_BaggingRegressor = BaggingRegressor()
####3.9ExtraTree极端随机树回归####
from sklearn.tree import ExtraTreeRegressor
model_ExtraTreeRegressor = ExtraTreeRegressor()

```
可以看到代码中用了很多回归的方法，但是我对于有些算法的理解在于很片面，以至于很是蒙逼，就比如KNN：在我的理解中这个是一个用于做分类的算法，怎就用于回归了呢？？？

## 区别

李航《统计学习方法》的解释为：
>输入变量与输出变量均为连续变量的预测问题是回归问题

我觉得这个好像有点问题，输入变量可能使离散的
>输出变量为有限个离散变量的预测问题为分类问题

[分类模型和回归模型本质一样：](https://www.cnblogs.com/pythoner6833/p/9296035.html#autoid-1-1-0)

1. 分类模型是将回归模型的输出离散化

2. 回归模型也可以将分类模型的输出连续化

回归问题通常是用来预测一个值，如预测房价、未来的天气情况等等，回归是对真实值的一种逼近预测。

定量输出称为回归，或者说是连续变量预测，预测明天的气温是多少度，这是一个回归任务

分类问题是用于将事物打上一个标签，通常结果为离散值。例如判断一幅图片上的动物是一只猫还是一只狗。分类并没有逼近的概念，最终正确结果只有一个，错误的就是错误的，不会有相近的概念。

定性输出称为分类，或者说是离散变量预测，预测明天是阴、晴还是雨，就是一个分类任务


## 各类算法原理

### [决策树](https://blog.csdn.net/hy592070616/article/details/81628956)

回归树与分类树最大的不同就在于节点的创建上

回归树：对于回归树用平方误差和最小

分类树：信息增益，信息增益比，基尼系数

#### 决策树分类

分类树是一种描述对实例进行分类的树形结构。在使用分类树进行分类时，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点。这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分到叶结点的类中。

#### 决策树回归

回归树是根据特征向量来决定对应的输出值。回归树就是将特征空间划分成若干单元，每一个划分单元有一个特定的输出。因为每个结点都是“是”和“否”的判断，所以划分的边界是平行于坐标轴的。对于测试数据，我们只要按照特征将其归到某个单元，便得到对应的输出值

### [SVM](http://zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA)回归

#### SVM

感觉这个东西写一篇文章也讲不完，主要就是找到支持向量，不能分就用核函数映射到高维空间。<br>网上找了张图片说明SVM和SVR的区别简单明了
![EptxKK.jpg](https://s2.ax1x.com/2019/04/19/EptxKK.jpg)
SVM目标函数：
$$\min _{w, b, \zeta} \frac{1}{2} w^{T} w+C \sum_{i=1}^{n} \zeta_{i}\\
subject \quad to\\
y_{i}\left(w^{T} \phi\left(x_{i}\right)+b\right) \geq 1-\zeta_{i}
\zeta_{i}>0, i=1, \ldots n
$$

#### SVR(Support Vector Regression)

x是向量，y是标量，拟合的函数形式为y=W^T*g(x)+b，其中g(x)为核函数对应的特征空间向量。
SVR认为，只要估计的y在实际的y的两侧一个固定的范围(epsilon)之内，就认为是估计正确，没有任何损失；
SVR目标函数：
$$\min _{w, b, \zeta, \zeta^{-}} \frac{1}{2} w^{T} w+C \sum_{i=1}^{n}\left(\zeta_{i}+\zeta_{i}^{*}\right)\\
subject \quad to:\\
y_{i}-w^{T} \phi\left(x_{i}\right)-b \leq \varepsilon+\zeta_{i}\\
w^{T} \phi\left(x_{i}\right)+b-y_{i} \leq \varepsilon+\zeta_{i}
\zeta_{i}, \zeta_{i}^{*} \geq 0, i=1, \ldots, n
$$

### KNN

KNN做回归和分类的主要区别在于最后做预测时候的决策方式不同

#### KNN回归

KNN做回归时，一般是选择平均法，即最近的K个样本的样本输出的平均值作为回归预测值。

#### KNN分类

KNN做分类预测时，一般是选择多数表决法，即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。

### 朴素贝叶斯

一直学的朴素贝叶斯就是一个用于分类的算法，而且人家是基于概率的模型进行分类，至今还记忆犹新的就是问什么叫朴素贝叶斯：因为在建模之前就假设了变量之间的独立性。所以就叫朴素贝叶斯了。

#### 分类

$y$是离散的类别，所以得到离散的 $p(y|x)$，给定 $x$ ，输出每个类上的概率
#### 回归
对上面离散的 $p(y | x)$求期望$\sum p(y|x)$ 就得到连续值。
还是一个求平均的问题。

### 神经网络

#### 回归

假设最后一层有$m$个神经元，每个神经元输出一个标量，$m$个神经元的输出可以看做向量 $v$，现全部连到一个神经元上，则这个神经元输出 $wv+b$，是一个连续值，可以处理回归问题，跟Linear Regression思想一样

#### 分类

假设现在这 $m$ 个神经元最后连接到 $N$ 个神经元，就有 $N$ 组 $w$ 值不同的 $wv+b$ ，同理可以归一化（比如用 softmax ）变成 $N$ 个类上的概率（补充一下，如果不用 softmax，而是每个 $wx+b$ 用一个 sigmoid，就变成多分类问题。

### [Boosting](https://www.cnblogs.com/pinard/p/6131423.html)

![EptO81.png](https://s2.ax1x.com/2019/04/19/EptO81.png)
Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

#### Boosting回归

对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干个弱学习器的输出进行平均得到最终的预测输出。

#### Boosting分类

对于分类问题的预测，我们通常使用的是投票法。<br>当初学习的时候我记得是先学的决策树，然后再学了这个算法，所以心里就有了个先后顺序，也不知道他们谁先谁后，但是跟决策树分类和回归差不多，都是平均和投票的方式。
AdaBoost是Boosting系列算法里最著名算法之一，原理基本也差不多。
后面的Bagging，随机森林等基本是基于决策树发展起来的，所以他们基本思想是一致的
