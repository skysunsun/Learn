
# [来源](https://mp.weixin.qq.com/s/hf9giCnOYINg_JXm8x5B7Q)


神经网络通过大量的参数模拟各种繁多的任务，并能拟合各种复杂的数据集。这种独特的能力使其能够在许多难以在“传统”机器学习时代取得进展的领域——例如图像识别、物体检测或自然语言处理等领域表现优异。

然而，有时候，最大的优点也是潜在的弱点。模型在学习过程时，如果缺乏控制可能会导致过拟合（overfitting）现象的发生——神经网络模型在训练集上表现很好，但对新数据预测时效果不好。因此，学习如何处理过拟合对于掌握机器学习至关重要。

近日，一位机器学习工程师提出了解决机器学习模型过拟合的6个必备技巧。



机器学习的基本问题是优化和泛化之间的张力。
 
优化是指调整模型，使其在训练数据(机器学习中的学习)上获得最佳性能的过程，而泛化是指经过训练后的模型在未见过的数据(测试集)上的表现如何。
 
训练的目标是得到良好的泛化。但是，研究者不能控制泛化，只能根据模型的训练数据调整模型。
 
如何知道模型是否过拟合?
 
过拟合的明显标志是在训练集中模型精度较高，但在新的数据或测试集中模型精度明显下降。这意味着该模型对训练数据非常了解，但不能泛化。这种情况使模型在生产或AB测试中的大多数领域无用。
 
如何防止过拟合?
 
幸运的是，有许多方法可以防止模型过拟合，下面描述一些最广泛使用的过拟合解决方案。
 
1. 缩小网络规模
 
防止过拟合最简单的方法是缩小模型的大小：模型中可学习参数的数量(由层数和每层的单元数决定)。
 
2. 交叉验证
 
在交叉验证中，将初始训练数据用作小的训练-测试拆分。
 
然后，将这些拆分用于调整模型。交叉验证最流行的形式是K-fold交叉验证。K表示折叠次数。
 
3. 增加权重正则化
 
给定两种解释，最有可能是正确的解释是最简单的一种，即做出较少假设的解释。
 
这个想法也适用于神经网络学习的模型：给定一些训练数据和一个网络架构，多组权重值可以解释数据。
 
与复杂的模型相比，简单的模型不太可能过拟合。
 
在这种情况下，一个简单的模型是参数值分布的熵较小的模型（或参数较少的模型）。
 
因此，减轻过拟合的一种常见方法是通过强制网络的权重仅采用较小的值来限制网络的复杂性，这使得权重值的分配更加规则，这被称为权重正则化，它是通过给网络的损失函数增加一个权重较大的成本来实现的。
 
这样的成本有两种:
 
L1正则化——增加的成本与权重系数的绝对值成正比。
 
L2正则化——增加的成本与权重系数值的平方成正比。
 
L2正则化在神经网络中也称为权值衰减。
 
4. 删除无关的特性
 
通过删除不相关的特性来改进数据。数据集可能包含许多对预测贡献不大的特性。
 
去除那些不太重要的特征可以提高准确性并减少过拟合。
 
可以通过使用scikit-learn的特性选择模块来完成。
 
5. 添加Dropout
 
应用于图层的Dropout包括退出包括在训练过程中随机退出（设置为零）该层的多个输出特性。
 
假设在训练过程中，给定的层通常会为给定的输入样本返回一个向量
[0.2,0.5,1.3,0.8,1.1]。

在应用Dropout之后，这个向量将有一些随机分布的零项:例如，[0,0.5,1.3,0,1.1]。
 
6. 数据增强
 
减少过拟合最简单的方法是增加训练数据的大小。假设我们处理的是图像，在这种情况下，有几种方法可以增加训练数据的大小——旋转图像、翻转、缩放、移动等等，这被称为数据增强，通常可以极大提高模型的准确性。

