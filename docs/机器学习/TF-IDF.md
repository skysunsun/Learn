# TF-IDF

TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。


## 计算词频：


$$\text{词频(TF)} =\frac{\text { 某个词在文章中的出现次数 }}{\text { 文章的总词数 }}$$

## 计算逆文档频率：

$$\text{逆文档频率(IDF)} =\log \left(\frac{\text { 文档总数 }}{\text { 包含该词的文档数 } +1}\right)$$


如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。取对数主要防止值太大

## 计算TF-IDF：

当有TF(词频)和IDF(逆文档频率)后，将这两个值相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。

$$\mathrm{TF}-\mathrm{IDF}= 词频(TF) \times 逆文档频率（IDF )$$


## 优缺点

TF-IDF的优点是简单快速，而且容易理解。

缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。



# fasttext

将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。


## subword n-gram

fastText引入了subword n-gram的概念来解决词形变化(morphology)的问题。直观上，它将一个单词打散到字符级别，并且利用字符级别的n-gram信息来捕捉字符间的顺序关系，希望能够以此丰富单词内部更细微的语义。我们知道，西方语言文字常常通过前缀、后缀、字根来构词，汉语也有单字表义的传统，所以这样的做法听起来还是有一定的道理。


从实验效果来看，subword n-gram信息的加入，不但解决了低频词未登录词的表达的问题，而且对于最终任务精度一般会有几个百分点的提升。唯一的问题就是由于需要估计的参数多，模型可能会比较膨胀。不过，Facebook也提供了几点压缩模型的建议：

· 采用hash-trick。由于n-gram原始的空间太大，可以用某种hash函数将其映射到固定大小的buckets中去，从而实现内存可控；

· 采用quantize命令，对生成的模型进行参数量化和压缩；

· 减小最终向量的维度。

需要注意的是以上几种方法都会以一定的精度损失为代价，尤其是维度的压缩，具体可以实践中再权衡。

## Hierarchical Softmax


Softmax大家都比较熟悉，它是逻辑回归(logisticregression)在多分类任务上的推广，是我们训练的神经网络中的最后一层。一般地，Softmax以隐藏层的输出h为输入，经过线性和指数变换后，再进行全局的归一化处理，找到概率最大的输出项。当词汇数量V较大时（一般会到几十万量级），Softmax计算代价很大，是O(V)量级。

层次化的Softmax的思想实质上是将一个全局多分类的问题，转化成为了若干个二元分类问题，从而将计算复杂度从O(V)降到O(logV)。

每个二元分类问题，由一个基本的逻辑回归单元来实现。如下图所示，从根结点开始，每个中间结点（标记成灰色）都是一个逻辑回归单元，根据它的输出来选择下一步是向左走还是向右走。下图示例中实际上走了一条“左-左-右”的路线，从而找到单词w₂。而最终输出单词w₂的概率，等于中间若干逻辑回归单元输出概率的连乘积。


## 和word2vec区别

1. 输入层：CBOW的输入是目标单词的上下文并进行one-hot编码，fastText的输入是多个单词embedding向量，并将单词的字符级别的n-gram向量作为额外的特征；

2. 从输入层到隐藏层，CBOW会将上下文单词向量叠加起来并经过一次矩阵乘法（线性变化）并应用激活函数，而fastText省略了这一过程，直接将embedding过的向量特征求和取平均；

3. 输出层，一般的CBOW模型会采用Softmax作为输出，而fastText则采用了Hierarchical Softmax，大大降低了模型训练时间；

4. CBOW的输出是目标词汇，fastText的输出是文档对应的类标。