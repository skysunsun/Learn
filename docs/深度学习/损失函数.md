

# 平方

平方损失函数是光滑函数，能够使用梯度下降法优化。然而当预测值距离真实值越远时，平方损失函数的惩罚力度越大，因此对异常点比较敏感。

$$Loss = \frac{1}{2}(y-\hat{y})^2$$
假设我们用$sigmoid$激活函数，并且使用平方损失函数：
$$\hat{y}=\sigma(wx+b)$$
现在用平方$Loss$对$W$进行求导：
$$
\begin{aligned}
\frac{\partial L}{\partial w} &=\frac{1}{2}2(y-\hat{y})\hat{y}^{\prime}\\
&= (y-\frac{1}{1+e^{-(wx+b)}})(\frac{1}{1+e^{-(wx+b)}})^{\prime}\\
&= (\frac{1}{1+e^{-(wx+b)}}-y)\frac{e^{-(wx+b)}x}{(1+e^{-(wx+b)})^2}\\
&= (\frac{1}{1+e^{-(wx+b)}}-y)x\frac{1}{1+e^{-(wx+b)}}\frac{e^{-(wx+b)}}{1+e^{-(wx+b)}}\\
&= (\frac{1}{1+e^{-(wx+b)}}-y)x\sigma(wx+b)(1-\sigma(wx+b))\\
&= (\hat{y}-y)x\sigma^{\prime}(wx+b)
\end{aligned}
$$
同样可以对平方$Loss$的$b$进行求导：
$$
\frac{\partial L}{\partial b}=(\hat{y}-y) \sigma^{\prime}(wx+b)
$$
可以看到不管是$W$还是$b$的导数都是和$sigmoid$函数的导数呈现正相关的关系，但是从$sigmoid$函数的图像上可以看到在函数的两端导数趋于0

这就会导致一个严重的问题：***梯度消失***

```python
torch.nn.MSELoss(input, target)
```

# 绝对值


绝对损失函数相当于在做中值回归，相比做均值回归的平方损失函数，绝对损失函数对异常点更鲁棒。但是，绝对损失函数在f=y处无法求导。


# Huber

Huber损失函数在|f-y|较小时为平方损失，在|f-y|较大的时采用线性损失，处处可导，且对异常点鲁棒。

# Hinge

Hinge损失函数是0-1损失函数相对紧的凸上界，且当时候,该函数不对其做任何处罚。由于Hinge损失在f.y=1处不可导，因此不能使用梯度下降算法优化，而是使用次梯度下降法。


# 交叉熵

交叉熵损失函数是常用的二分类损失函数。交叉熵损失函数也是0-1损失的光滑凸上界。


$$
Loss=-y \ln \hat{y}-(1-y) \ln (1-\hat{y})
$$
和上面一样对$W$和$b$求导：
$$
\begin{aligned} \frac{\partial L}{\partial w} &=-{y\frac{1}{\hat{y}}\hat{y}^\prime-(1-y)\frac{1}{1-\hat{y}}(1-\hat{y})^\prime}\\
&= -\left(\frac{y}{\sigma(wx+b)}-\frac{(1-y)}{1-\sigma(wx+b)}\right) \frac{\partial \sigma}{\partial w} \\
&=-\left(\frac{y}{\sigma(wx+b)}-\frac{(1-y)}{1-\sigma(wx+b)}\right) \sigma^{\prime}(wx+b) x \\
&= \frac{\sigma^{\prime}(wx+b) x}{\sigma(wx+b)(1-\sigma(wx+b))}(\sigma(wx+b)-y) \\
&= x(\sigma(wx+b)-y)\\
&=x(\hat{y}-y)
\end{aligned}
$$
>注:因为sigmoid的导数为$\sigma(x)(1-\sigma(x)$,所以第一项直接抵消掉了

同样可以求得$L$对$b$的导数为：
$$
\frac{\partial L}{\partial b}=(\hat{y}-y)
$$
可以看到这就和激活函数的导数没有关系了，也就不存在梯度消失的问题了！

现在假设使用$softmax$函数作为激活函数，对损失函数的梯度进行推导：
因为对于多标签来说，只有一个是对的，也就是说只有一个的值为1，那么交叉熵的公式就简化为：
$$
Loss=-y \ln \hat{y}
$$
由于$y=1$,进一步简化得：
$$
Loss=-\ln \hat{y}
$$
对这个进行求导：
$$
\frac{\partial L}{\partial w}=-\frac{1}{\hat{y}}\hat{y}\prime
$$
主要是集中在后一项的求导上面！

```python
torch.nn.BCEloss(input, target)
```

[熵-KL散度-交叉熵](https://blog.csdn.net/Dby_freedom/article/details/83374650)


# [Dice_loss](https://arxiv.org/pdf/1606.04797.pdf)

[参考](./docs/论文笔记/Dice_Loss.md)

# [Focal_loss](https://arxiv.org/pdf/1708.02002.pdf)
主要是为了解决样本不均衡的问题而提出来的

考虑二分类的交叉熵
$$L_{c e}=-y \log \hat{y}-(1-y) \log (1-\hat{y})=\left\{\begin{array}{l}
-\log (\hat{y}) &\text { if } y=1 \\
-\log (1-\hat{y}) &\text { if } y=0
\end{array}\right.$$

如果正负样本的数量差距很大，**那么一种常见的做法就是给正负样本加上权重**，负样本出现的频次多，那么就降低负样本的权重，正样本数量少，就相对提高正样本的权重。因此可以通过设定$-\alpha$的值来控制正负样本对总的$loss$的共享权重。$-\alpha$取比较小的值来降低负样本（多的那类样本）的权重。
$$L_{f l}=\left\{\begin{array}{l}
-\alpha(1-\hat{y})\log \hat{y},&\text { if } y=1 \\
-(1-\alpha) \hat{y}\log (1-\hat{y})& \text { if } y=0
\end{array}\right.$$

上面的公式虽然可以控制正负样本的权重，但是没法控制容易分类和难分类样本的权重，因此加入另一个参数$\gamma$

$$L_{f l}=\left\{\begin{array}{l}
-\alpha(1-\hat{y})^{\gamma} \log \hat{y}&\text { if } y=1 \\
-(1-\alpha)\hat{y}^{\gamma} \log (1-\hat{y})&\text { if } y=0
\end{array}\right.$$
## [代码](https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py)
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

class FocalLoss(nn.Module):
    def __init__(self, gamma=0, alpha=None, size_average=True):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        if isinstance(alpha,(float,int,long)): self.alpha = torch.Tensor([alpha,1-alpha])
        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)
        self.size_average = size_average

    def forward(self, input, target):
        if input.dim()>2:
            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W
            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C
            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C
        target = target.view(-1,1)

        logpt = F.log_softmax(input)
        logpt = logpt.gather(1,target)
        logpt = logpt.view(-1)
        pt = Variable(logpt.data.exp())

        if self.alpha is not None:
            if self.alpha.type()!=input.data.type():
                self.alpha = self.alpha.type_as(input.data)
            at = self.alpha.gather(0,target.data.view(-1))
            logpt = logpt * Variable(at)

        loss = -1 * (1-pt)**self.gamma * logpt
        if self.size_average: return loss.mean()
        else: return loss.sum()
```

# [Iou_loss]((https://github.com/NathanUA/BASNet/tree/master/pytorch_iou))

```python
import torch
import torch.nn.functional as F
from torch.autograd import Variable
import numpy as np

def _iou(pred, target, size_average = True):

    b = pred.shape[0]
    IoU = 0.0
    for i in range(0,b):
        #compute the IoU of the foreground
        Iand1 = torch.sum(target[i,:,:,:]*pred[i,:,:,:])
        Ior1 = torch.sum(target[i,:,:,:]) + torch.sum(pred[i,:,:,:])-Iand1
        IoU1 = Iand1/Ior1

        #IoU loss is (1-IoU1)
        IoU = IoU + (1-IoU1)

    return IoU/b

class IOU(torch.nn.Module):
    def __init__(self, size_average = True):
        super(IOU, self).__init__()
        self.size_average = size_average

    def forward(self, pred, target):

        return _iou(pred, target, self.size_average)

```

# [SSIM_loss]((https://github.com/Po-Hsun-Su/pytorch-ssim))

```python
import torch
import torch.nn.functional as F
from torch.autograd import Variable
import numpy as np
from math import exp

def gaussian(window_size, sigma):
    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])
    return gauss/gauss.sum()

def create_window(window_size, channel):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())
    return window

def _ssim(img1, img2, window, window_size, channel, size_average = True):
    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)
    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)

    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1*mu2

    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq
    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq
    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2

    C1 = 0.01**2
    C2 = 0.03**2

    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))

    if size_average:
        return ssim_map.mean()
    else:
        return ssim_map.mean(1).mean(1).mean(1)

class SSIM(torch.nn.Module):
    def __init__(self, window_size = 11, size_average = True):
        super(SSIM, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = 1
        self.window = create_window(window_size, self.channel)

    def forward(self, img1, img2):
        (_, channel, _, _) = img1.size()

        if channel == self.channel and self.window.data.type() == img1.data.type():
            window = self.window
        else:
            window = create_window(self.window_size, channel)
            
            if img1.is_cuda:
                window = window.cuda(img1.get_device())
            window = window.type_as(img1)
            
            self.window = window
            self.channel = channel


        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)

def ssim(img1, img2, window_size = 11, size_average = True):
    (_, channel, _, _) = img1.size()
    window = create_window(window_size, channel)
    
    if img1.is_cuda:
        window = window.cuda(img1.get_device())
    window = window.type_as(img1)
    
    return _ssim(img1, img2, window, window_size, channel, size_average)
```

# AP-Loss













