

# 梯度下降法(Gradient Descent)

梯度下降法是最基本的一类优化器，目前主要分为三种梯度下降法：标准梯度下降法(GD, Gradient Descent)，随机梯度下降法(SGD, Stochastic Gradient Descent)及批量梯度下降法(BGD, Batch Gradient Descent)。

## 标准梯度下降法(GD)

假设要学习训练的模型参数为$W$，代价函数为$J(W)$，则代价函数关于模型参数的偏导数即相关梯度为$\Delta J(W)$，学习率为$\eta_{t}$，则使用梯度下降法更新参数为：
$$
W_{t+1}=W_{t}-\eta_{t} \Delta J\left(W_{t}\right)
$$
其中，$W_{t}$表示$t$时刻的模型参数。从表达式来看，模型参数的更新调整，与代价函数关于模型参数的梯度有关，即沿着梯度的方向不断减小模型参数，从而最小化代价函数。基本策略可以理解为”在有限视距内寻找最快路径下山“，因此每走一步，参考当前位置最陡的方向(即梯度)进而迈出下一步。
### 标准梯度下降法主要有两个缺点:

1. 训练速度慢：每走一步都要要计算调整下一步的方向，下山的速度变慢。在应用于大型数据集中，每输入一个样本都要更新一次参数，且每次迭代都要遍历所有的样本。会使得训练过程及其缓慢，需要花费很长时间才能得到收敛解。
2. 容易陷入局部最优解：由于是在有限视距内寻找下山的反向。当陷入平坦的洼地，会误以为到达了山地的最低点，从而不会继续往下走。所谓的局部最优解就是鞍点。落入鞍点，梯度为0，使得模型参数不在继续更新。

## 批量梯度下降法(BGD)
假设批量训练样本总数为$n$，每次输入和输出的样本分别为$X^{(i)}, Y^{(i)}$，模型参数为$W$，代价函数为$J(W)$，每输入一个样本$i$代价函数关于$W$的梯度为$\Delta J_{i}\left(W_{t}, X^{(i)}, Y^{(i)}\right)$，学习率为$\eta_{t}$，则使用批量梯度下降法更新参数表达式为：
$$
W_{t+1}=W_{t}-\eta_{t} \sum_{i=1}^{n} \Delta J_{i}\left(W_{t}, X^{(i)}, Y^{(i)}\right)
$$
其中，$W_{t}$表示$t$时刻的模型参数。从表达式来看，模型参数的调整更新与全部输入样本的代价函数的和（即批量/全局误差）有关。即每次权值调整发生在批量样本输入之后，而不是每输入一个样本就更新一次模型参数。这样就会大大加快训练速度。基本策略可以理解为，在下山之前掌握了附近的地势情况，选择总体平均梯度最小的方向下山。批量梯度下降法比标准梯度下降法训练时间短，且每次下降的方向都很正确。

## 随机梯度下降法(SGD)
假设批量训练样本总数为$n$，每次输入和输出的样本分别为$X^{(i)}, Y^{(i)}$，模型参数为$W$，代价函数为$J(W)$，则使用随机梯度下降法更新参数表达式为：
$$
W_{t+1}=W_{t}-\eta_{t} g_{t}
$$
其中$g_{t}=\Delta J_{i_{s}}\left(W_{t} ; X^{\left(i_{s}\right)} ; Y^{\left(i_{s}\right)}\right), \quad i_{s} \in\{1,2, \ldots, n\}$表示随机选择的一个梯度方向，$W_{t}$表示$t$时刻的模型参数。$E\left(g_{t}\right)=\Delta J\left(W_{t}\right)$，这里虽然引入了随机性和噪声，但期望仍然等于正确的梯度下降。基本策略可以理解为随机梯度下降像是一个盲人下山，不用每走一步计算一次梯度，但是他总能下到山底，只不过过程会显得扭扭曲曲。

### 优点

1. 虽然SGD需要走很多步的样子，但是对梯度的要求很低（计算梯度快）。而对于引入噪声，大量的理论和实践工作证明，只要噪声不是特别大，SGD都能很好地收敛。
2. 应用大型数据集时，训练速度很快。比如每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。相比于标准梯度下降法的遍历全部样本，每输入一个样本更新一次参数，要快得多。

### 缺点

1. SGD在随机选择梯度的同时会引入噪声，使得权值更新的方向不一定正确。
2. 此外，SGD也没能单独克服局部最优解的问题。

# 动量优化法
动量优化方法是在梯度下降法的基础上进行的改变，具有加速梯度下降的作用。一般有标准动量优化方法Momentum、NAG（Nesterov accelerated gradient）动量优化方法。

## Momentum

使用动量(Momentum)的随机梯度下降法(SGD)，主要思想是引入一个积攒历史梯度信息动量来加速SGD。从训练集中取一个大小为$n$的小批量$\left\{X^{(1)}, X^{(2)}, \ldots, X^{(n)}\right\}$样本，对应的真实值分别为$Y^{(i)}$，则Momentum优化表达式为：
$$
\left\{\begin{array}{l}{v_{t}=\alpha v_{t-1}+\eta_{t} \Delta J\left(W_{t}, X^{\left(i_{s}\right)}, Y^{\left(i_{s}\right)}\right)} \\ {W_{t+1}=W_{t}-v_{t}}\end{array}\right.
$$
其中，$v_{t}$表示$t$时刻积攒的加速度。$\alpha$表示动力的大小，一般取值为0.9（表示最大速度10倍于SGD）。$\Delta J\left(W_{t}, X^{\left(i_{s}\right)}, Y^{\left(i_{s}\right)}\right)$含义见SGD算法。$W_{t}$表示$t$时刻的模型参数。基本策略可以理解为：

由于当前权值的改变会受到上一次权值改变的影响，类似于小球向下滚动的时候带上了惯性。这样可以加快小球向下滚动的速度。动量主要解决SGD的两个问题：
1. 随机梯度的方法（引入的噪声）
2. Hessian矩阵病态问题（可以理解为SGD在收敛过程中和正确梯度相比来回摆动比较大的问题）

## NAG

牛顿加速梯度（NAG, Nesterov accelerated gradient）算法，是Momentum动量算法的变种。更新模型参数表达式如下：
$$
\left\{\begin{array}{l}{v_{t}=\alpha v_{t-1}+\eta_{t} \Delta J\left(W_{t}-\alpha v_{t-1}\right)} \\ {W_{t+1}=W_{t}-v_{t}}\end{array}\right.
$$
其中，$v_{t}$表示$t$时刻积攒的加速度。$\alpha$表示动力的大小，$\eta_{t}$表示学习率；$W_{t}$表示$t$时刻的模型参数，$\Delta J\left(W_{t}-\alpha v_{t-1}\right)$表示代价函数关于$W_{t}$的梯度。Nesterov动量梯度的计算在模型参数施加当前速度之后，因此可以理解为往标准动量中添加了一个校正因子。基本策略可以理解为：

在Momentun中小球会盲目地跟从下坡的梯度，容易发生错误。所以需要一个更聪明的小球，能提前知道它要去哪里，还要知道走到坡底的时候速度慢下来而不是又冲上另一个坡。计算$W_{t}-\alpha v_{t-1}$可以表示小球下一个位置大概在哪里。从而可以提前知道下一个位置的梯度，然后使用到当前位置来更新参数。
在凸批量梯度的情况下，Nesterov动量将额外误差收敛率从$O(1 / k)$($k$步后)改进到$O\left(1 / k^{2}\right)$。然而，在随机梯度情况下，Nesterov动量对收敛率的作用却不是很大。

# 自适应学习率优化算法
自适应学习率优化算法针对于机器学习模型的学习率，传统的优化算法要么将学习率设置为常数要么根据训练次数调节学习率。极大忽视了学习率其他变化的可能性。然而，学习率对模型的性能有着显著的影响，因此需要采取一些策略来想办法更新学习率，从而提高训练速度。 目前的自适应学习率优化算法主要有：AdaGrad算法，RMSProp算法，Adam算法以及AdaDelta算法。

## AdaGrad算法
AdaGrad算法，独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平均值总和的平方根。具有代价函数最大梯度的参数相应地有个快速下降的学习率，而具有小梯度的参数在学习率上有相对较小的下降。
$$
W_{t+1}=W_{t}-\frac{\eta_{0}}{\sqrt{\sum_{t^{\prime}=1}^{t}\left(g_{t^{\prime}, i}\right)+\epsilon}} \odot g_{t, i}
$$
假定一个多分类问题，$i$表示第$i$个分类，$t$表示第$t$次迭代同时也表示分类$i$累计出现的次数。$\eta_{0}$表示初始的学习率取值一般为0.01，$\epsilon$是一个取值很小的数（一般为1e-8）为了避免分母为0。$W_{t}$表示$t$时刻的模型参数，$g_{t, i}=\Delta J\left(W_{t, i}\right)$表示$t$时刻，指定分类$i$，代价函数$J(⋅)$关于$W$的梯度。
从表达式可以看出，对出现比较多的类别数据，Adagrad给予越来越小的学习率，而对于比较少的类别数据，会给予较大的学习率。因此Adagrad适用于数据稀疏或者分布不平衡的数据集。
Adagrad 的主要优势在于不需要人为的调节学习率，它可以自动调节；缺点在于，随着迭代次数增多，学习率会越来越小，最终会趋近于0。

## RMSProp算法
RMSProp算法修改了AdaGrad的梯度积累为指数加权的移动平均，使得其在非凸设定下效果更好。
$$
\left\{\begin{array}{l}{E\left[g^{2}\right]_{t}=\alpha E\left[g^{2}\right]_{t-1}+(1-\alpha) g_{t}^{2}} \\ {W_{t+1}=W_{t}-\frac{\eta_{0}}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} \odot g_{t}}\end{array}\right.
$$

其中，$\boldsymbol{W}_{t}$表示$t$时刻模型的参数，$g_{t}=\Delta J\left(W_{t}\right)$表示$t$次迭代代价函数关于$W$的梯度大小，$E\left[g^{2}\right]_{t}$表示前$t$次的梯度平方的均值。$\alpha$表示动力（通常设置为0.9），$\eta_{0}$表示全局初始学习率。$\epsilon$是一个取值很小的数（一般为1e-8）为了避免分母为0。RMSProp借鉴了Adagrad的思想，观察表达式，分为$\sqrt{E\left[g^{2}\right]_{t}+\epsilon}$。由于取了个加权平均，避免了学习率越来越低的的问题，而且能自适应地调节学习率。
RMSProp算法在经验上已经被证明是一种有效且实用的深度神经网络优化算法。目前它是深度学习从业者经常采用的优化方法之一。

## AdaDelta算法
AdaGrad算法和RMSProp算法都需要指定全局学习率，AdaDelta算法结合两种算法每次参数的更新步长即：
$$
\begin{aligned} \Delta W_{\text {AdaGrad,t}} &=-\frac{\eta_{0}}{\sqrt{\sum_{t^{\prime}=1}^{t}\left(g_{t^{\prime}, i}\right)+\epsilon}} \odot g_{t} \\ \Delta W_{\text {RMSProp}, t} &=-\frac{\eta_{0}}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} \odot g_{t} \end{aligned}
$$
AdaDelta算法策略可以表示为：
$$
\left\{\begin{array}{l}{E\left[g^{2}\right]_{t}=\alpha E\left[g^{2}\right]_{t-1}+(1-\alpha) g_{t}^{2}} \\ {\Delta W_{t}=-\frac{\sqrt{\sum_{i=1}^{t-1} \Delta W_{i}}}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}}} \\ {W_{t+1}=W_{t}+\Delta W_{t}}\end{array}\right.
$$
其中${W}_{t}$表示$t$时刻模型的参数，$g_{t}=\Delta J\left(W_{t}\right)$表示$t$次迭代代价函数关于$W$的梯度大小。$E\left[g^{2}\right]_{t}$表示前$t$次的梯度平方的均值。$\sum_{i=1}^{t-1} \Delta W_{i}$表示前$t−1$次模型参数每次的更新步长累加求根。从表达式可以看出，AdaDelta不需要设置一个默认的全局学习率。在模型训练的初期和中期，AdaDelta表现很好，加速效果不错，训练速度快。
在模型训练的后期，模型会反复地在局部最小值附近抖动。

## Adam算法
它的名称来源于适应性矩估计（adaptive moment estimation）。在介绍这个算法时，原论文列举了将 Adam 优化算法应用在非凸优化问题中所获得的优势：
1. 直截了本地实现
2. 高效的计算
3. 所需内存少
4. 梯度对角缩放的不变性
5. 适合解决含大规模数据和参数的优化问题
6. 适用于非稳态（non-stationary）目标
7. 适用于解决包含很高噪声或稀疏梯度的问题
8. 超参数可以很直观地解释，并且基本上只需极少量的调参

首先，Adam中动量直接并入了梯度一阶矩（指数加权）的估计。其次，相比于缺少修正因子导致二阶矩估计可能在训练初期具有很高偏置的RMSProp，Adam包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩估计。
$$
\left\{\begin{array}{c}{m_{t}=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t}} \\ {v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}} \\ {\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}, \quad \hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}} \\ {W_{t+1}=W_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon} \hat{m}_{t}}\end{array}\right.
$$

其中，$m_{t}$和$v_{t}$分别为一阶动量项和二阶动量项。$\beta_{1}, \beta_{2}$为动力值大小通常分别取0.9和0.999；$\hat{m}_{t}, \hat{v}_{t}$分别为各自的修正值。${W}_{t}$表示$t$时刻模型的参数，$g_{t}=\Delta J\left(W_{t}\right)$表示$t$次迭代代价函数关于$W$的梯度大小；$\epsilon$是一个取值很小的数（一般为1e-8）为了避免分母为0。

（一阶矩为均值，二阶矩为方差）

### 优点
1. Adam通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。
2. 适应性梯度算法（AdaGrad）为每一个参数保留一个学习率以提升在稀疏梯度（即自然语言和计算机视觉问题）上的性能。
3. 均方根传播（RMSProp）基于权重梯度最近量级的均值为每一个参数适应性地保留学习率。这意味着算法在非稳态和在线问题上有很有优秀的性能。
