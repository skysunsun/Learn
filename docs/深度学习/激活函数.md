# 什么是激活函数？

在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数就是激活函数.

# 为什么需要激活函数？

如果不用激活函数，在这种情况下你每一层节点的输入都是上层输出的线性映射，那么无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，那么网络的逼近能力就相当有限。所以引入非线性函数作为激活函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。

# 常见的激活函数

## Sigmoid函数
### 函数及导数
$$sigmoid=\frac{1}{1+e^{-x}}$$
为了表述方便使用$\sigma$表示$sigmoid$函数,$sigmoid$函数的导数：
$$
\begin{aligned} \sigma^{\prime}(x) &=\left(\frac{1}{1+e^{-x}}\right)^{\prime} \\  &=\frac{1}{\left(1+e^{-x}\right)^{2}} \cdot\left(e^{-x}\right) \\ &=\frac{1}{1+e^{-x}} \cdot \frac{e^{-x}}{1+e^{-x}} ) \\ &=\sigma(x)(1-\sigma(x)) \end{aligned}
$$
### 图像


## Softmax函数
![ZdCGFS.png](https://s2.ax1x.com/2019/07/05/ZdCGFS.png)
### 函数及导数
$$
\sigma(\mathbf{z})_{j}=\frac{e^{\mathbf{z}_{j}}}{\sum_{k=1}^{K} e^{\mathbf{z}_k}}
$$
####  当$i=j$时
$$
\begin{aligned}
\frac{\partial y_{i}}{\partial x_{j}}&=\frac{\partial y_{i}}{\partial x_{i}}\\
 &=\frac{\partial}{\partial x_{i}}\left(\frac{e^{x_{i}}}{\sum_{k} e^{x_{k}}}\right) \\ &=\frac{\left(e^{x_{i}}\right)^{\prime}\left(\sum_{k} e^{x_{k}}\right)-e^{x_{i}}\left(\sum_{k} e^{x_{k}}\right)^{\prime}}{\left(\sum_{k} e^{x_{k}}\right)^{2}} \\ &=\frac{e^{x_{i}} \cdot\left(\sum_{k} e^{x_{k}}\right)-e^{x_{i}} \cdot e^{x_{i}}}{(\sum_{k} e^{x_{k}} )^{2}} \\
 &=\frac{e^{x_{i}}}{\sum_{k} e^{x_{k}}}-\frac{e^{x_{i}}}{\sum_{k} e^{x_{k}}} \cdot \frac{e^{x_{i}}}{\sum_{k} e^{x_{k}}} \\
 &=y_{i}-y_{i} \cdot y_{i} \\
 &=y_{i}\left(1-y_{i}\right) \end{aligned}
$$
#### 当$i \neq j$时
$$
\begin{aligned} \frac{\partial y_{i}}{\partial x_{j}} &=\frac{\partial}{\partial x_{j}}\left(\frac{e^{x_{i}}}{\sum_{k} e^{x_{k}}}\right) \\ &=\frac{\left(e^{x_{i}}\right)^{\prime}\left(\sum_{k} e^{x_{k}}\right)-e^{x_{i}}\left(\sum_{k} e^{x_{k}}\right)^{\prime}}{\left(\sum_{k} e^{x_{k}}\right)^{2}} \\ &=\frac{0 \cdot\left(\sum_{k} e^{x_{k}}\right)-e^{x_{i}} \cdot e^{x_{j}}}{\left(\sum_{k} e^{x_{k}}\right)^{2}} \\ &=\frac{-e^{x_{i}}}{\sum_{k} e^{x_{k}}} \cdot \frac{e^{x_{j}}}{\sum_{k} e^{x_{k}}} \\
&=-y_{i} \cdot y_{j} \end{aligned}
$$
所以
$$
\frac{\partial y_{i}}{\partial x_{j}}=\left\{\begin{array}{l}{y_{i}-y_{i} y_{i}},i=j
\\ {0-y_{i} \cdot y_{j}, i\neq j}\end{array}\right.
$$
### 图像

## $softmax$函数$vs$ $sigmoid$函数
1. $sigmoid$函数的计算是独立的，可以全都是0.99或者全都是0.01，而$softmax$函数是相关联的，一个大了另一个就会小
2.  $sigmoid$函数总和可能不为1，而$softmax$函数的总和肯定是为1的
3.  如果是二分类，那么这两个函数基本等价
### 优点
它能够把输入的连续实值变换为0和1之间的输出，特别适用于二分类任务
### 缺点

1. 在饱和区梯度接近零，根据反向传播链式法则(越乘越小)，会导致梯度消失
2. Sigmoid的输出不是以0为均值的。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是：如x>0, $f=w^Tx+b$,那么对$w$求局部梯度则都为正，这样在反向传播的过程中$w$要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。
3. 含有幂运算，比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。

## tanh函数
### 函数及导数
$$
Tanh(x)\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
$$
$$
Tanh'(x)=1-Tanh^2(x)
$$
### 图像
 
x
### 优点

Tanh的输出是以0为均值的

### 缺点

1. 在饱和区梯度接近零，根据反向传播链式法则(越乘越小)，会导致梯度消失

2. 含有幂运算，比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。	

## Relu函数
### 函数及导数(在零处不可导，使用次导数代替)
$$Relu=max(0,x)$$
$$
Relu'=\left\{
\begin{aligned}
0 \quad x\leq 0\\
1 \quad x> 0
\end{aligned}
\right.
$$

### 图像


### 优点

1. 计算快，只需要判断输入是否大于0；收敛块
2. 简单

### 缺点

1. Relu的输出不是以0为均值的。
2. Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。


## Leaky ReLU函数（PReLU）
### 函数及导数
$$f(x)=max(\alpha x,x)$$

### 图像

### 优点
人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为$αx$而非0，通常$α=0.01$另外一种直观的想法是基于参数的方法，即$ParametricReLU:f(x)=max(αx,x)$其中$α$可由方向传播算法学出来。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，
### 缺点
但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。

## ELU (Exponential Linear Units) 函数
### 函数及导数
$$
ELU'=\left\{
\begin{aligned}
α(e^x-1) \quad x\leq 0\\
x \quad x> 0
\end{aligned}
\right.
$$

### 图像	
 
### 优点
ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及：
不会有Dead ReLU问题
输出的均值接近0，zero-centered

### 缺点
它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。

## [MaxOut函数](https://arxiv.org/pdf/1302.4389.pdf)
### 函数及导数

$$
\begin{aligned}
h_{i}(x)=\max _{j \in[1, k]} z_{i j}\\
z_{ij}=W^T_{ij}x+b_{ij}
\end{aligned}
$$


## [Swish函数](https://arxiv.org/abs/1710.05941)

### 函数及导数
$$
f(x)=xSigmoid(βx)=\frac x{1+e^{-x}}
$$
$$
f'(x)=\beta f(x)+\frac{(\beta x)}{1+e^{-\beta x}}(1-\beta f(x))
$$
### 图像

### 优点

### 缺点


## [Mish函数](https://arxiv.org/ftp/arxiv/papers/1908/1908.08681.pdf)

### 函数及导数

$$\begin{aligned}
f(x)&=x \cdot \tanh \left(\ln \left(1+e^{x}\right)\right)\\
f'(x)&=\frac{e^{x} \omega}{\delta^{2}} 其中\\
\omega&=4(x+1)+4 e^{2 x}+e^{3 x}+e^{x}(4 x+6)\\
\delta&=2 e^{x}+e^{2 x}+2
\end{aligned}
$$
### 图像

### 优点

### 缺点

慢