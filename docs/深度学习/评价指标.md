# 混淆矩阵
![uIDvLD.png](https://s2.ax1x.com/2019/10/09/uIDvLD.png)
# Precision(越大越好)
**精确率：你的预测有多少是对的**
$$
P=\frac{A}{A+C}
$$
# Recall(越大越好)
**召回率：正例里你的预测覆盖了多少**
$$
R=\frac{A}{A+B}
$$
# PR曲线(越右上凸效果越好)
**PR(Precision Recall)曲线表现的是Precision和Recall之间的关系，且PR曲线完全聚焦于正例**

算法对样本进行分类时，都会有置信度，即表示该样本是正样本的概率,通过选择合适的阈值，比如50%，预测结果进行划分，概率大于50%的就认为是正例，小于50%的就是负例。从而得到每个阈值的Precision和Recall值，然后根据值，以 Recall 为横轴，Precision 为纵轴画图就能得到PR曲线
![NOT8fJ.png](https://s1.ax1x.com/2020/07/03/NOT8fJ.png)


# TPR(越大越好)
**灵敏度：它是所有实际正例中，正确识别的正例比例，同召回率**
$$
TPR=\frac{A}{A+B}
$$
# FPR(越小越好)
**(特异度：它是实际负例中，错误的识别为正例的负例比例)**
$$
FPR=\frac{B}{B+D}
$$
# receiver operating characteristic curve(越左上凸越好)

**ROC曲线：由于兼顾正例与负例，所以适用于评估分类器的整体性能(适用于样本不均衡问题)**

ROC曲线和PR曲线是一样的画法，不同点是ROC曲线是以 FPR 为横轴，TPR 为纵轴。

![NOTf78.png](https://s1.ax1x.com/2020/07/03/NOTf78.png)


# Area Under the Curve(越大越好)

**AUC：ROC曲线下的面积用于衡量“二分类问题”机器学习算法性能（泛化能力）**
AUC值实际上反映了模型的 rank 能力，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面。这个指标尤其适用在某些场景下(如 CTR 预估)，每次要返回的是最有可能点击的若干个广告(根据CTR排序, 选择排在前面的若干个)，实际上便是在考验模型的排序能力。除此之外，CTR 中存在着样本不均衡的问题，正负样本比例通常会大于 1:100，如果采用 PR 曲线，则会导致 AUC 发生剧变，无法较好反映模型效果。

算面积一般用积分，但是ROC曲线一般不光滑，而且没有具体的函数。所以换个方式计算AUC
$$A U C=\frac{\sum_{i \in \text { positiveclass }} \operatorname{rank}_{i}-\frac{M(1+M)}{2}}{M \times N}$$

它也是首先对score从大到小排序，然后令最大score对应的sample 的rank为n，第二大score对应sample的rank为n-1，以此类推。然后把所有的正类样本的rank相加，再减去M-1种两个正样本组合的情况。得到的就是所有的样本中有多少对正类样本的score大于负类样本的score。然后再除以M×N。即 

```python
import numpy as np
def calc_auc(y_labels, y_scores):
    f = list(zip(y_scores, y_labels))
    rank = [values2 for values1, values2 in sorted(f, key=lambda x:x[0])]
    rankList = [i+1 for i in range(len(rank)) if rank[i] == 1]
    pos_cnt = np.sum(y_labels == 1)
    neg_cnt = np.sum(y_labels == 0)
    auc = (np.sum(rankList) - pos_cnt*(pos_cnt+1)/2) / (pos_cnt*neg_cnt)
    print(auc)
```
# $F_{\beta},F_{0.5},F_{1},F_{2},$



$$
F_{\beta}=\left(1+\beta^{2}\right) \frac {P \cdot R}{\beta^{2} \cdot P+R}
$$


当$\beta=0.5$时，则为$F_{0.5}$，该分数认为召回率的重要程度是准确率的一半(很少看到用这个的)
$$
F_{0.5}=3/2\frac{P R}{1/2P+R}
$$
当$\beta=1$时，则为$F_{1}$，该分数认为召回率和准确率同等重要（常用啊）
$$
F_{1}=2\frac{P R}{P+R}
$$

当$\beta=\sqrt2$时，则为$F_{2}$，该分数认为召回率的重要程度是准确率的2倍（一些比赛会用到这个）
$$
F_{2}=3\frac{P R}{2P+R}
$$
在显著性检测任务中$\beta=\sqrt{0.3}$，为常用的评价指标




# Pixel Accuracy

**像素准确率(PA):把分对的像素总量除以像素总数**
$$P A=\frac{\sum_{i=0}^{k} p_{i i}}{\sum_{i=0}^{k} \sum_{j=0}^{k} p_{i j}}$$

# Mean Pixel Accuracy

**平均像素准确率(MPA):MPA是对PA的改进，它是先对每个类计算PA，然后再对所有类的PA求平均**
$$M P A=\frac{1}{k+1} \sum_{i=0}^{k} \frac{p_{i i}}{\sum_{j=0}^{k} p_{i j}}$$

# Intersection over union

**IOU(交并比)衡量的是两个区域的重叠程度，是两个区域重叠部分面积占二者总面积（重叠部分只计算一次）的比例**
在目标检测任务中，如果模型输出的矩形框与人工标注的矩形框的IoU值大于某个阈值时（通常为0.5）即认为模型输出了正确的

# mIOU

**mIoU是度量语义分割准确率的方法；分别对每个类计算（真实标签和预测结果的交并比）IOU，然后再对所有类别的IOU求均值**
$$m I o U=\frac{1}{k+1} \sum_{i=0}^{k} \frac{p_{i i}}{\sum_{j=0}^{k} p_{i j}+\sum_{j=0}^{k} p_{j i}-p_{i i}}$$
# Average Precision

**AP(平均精确率)是对PR曲线上的Precision值求均值。对于PR曲线来说，我们使用积分来进行计算。**
$$A P=\int_{0}^{1} p(r) d r$$
> 注：因为PR曲线并不是光滑的，所以需要PR曲线进行平滑处理

Iou的阈值从固定的0.5调整为在 0.5 - 0.95 的区间上每隔0.5计算一次AP的值，取所有结果的平均值作为最终的结果。

AP衡量的是学出来的模型在给定类别上的好坏
# mAP

多标签图像分类任务中图片的标签不止一个，因此评价不能用普通单标签图像分类的标准，即mean accuracy，该任务采用的是和信息检索中类似的方法—mAP（mean Average Precision）


mAP衡量的是学出的模型在所有类别上的好坏


# [Structural Similarity](http://www.cns.nyu.edu/pub/lcv/wang03-preprint.pdf)

**结构相似性(SSIM):用于评价两幅图像相似度的指标**

$$\operatorname{SSIM}(\mathrm{x}, \mathrm{y})=\frac{\left(2 \mu_{\mathrm{x}} \mu_{\mathrm{y}}+C_{1}\right)\left(2 \sigma_{\mathrm{xy}}+C_{2}\right)}{\left(\mu_{\mathrm{x}}^{2}+\mu_{\mathrm{y}}^{2}+C_{1}\right)\left(\sigma_{\mathrm{x}}^{2}+\sigma_{\mathrm{y}}^{2}+C_{2}\right)}$$

[Pytorch代码](https://github.com/Po-Hsun-Su/pytorch-ssim/blob/master/pytorch_ssim/__init__.py)


