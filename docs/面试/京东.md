# CV
卷积(傅里叶变换)

是通过两个函数f 和g 生成第三个函数的一种数学算子，表征函数f 与g经过翻转和平移的重叠部分函数值乘积对重叠长度的积分。

**局部连接**
这个是最容易想到的，每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。
**权值共享**
一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。
**下采样**
Pooling层利用图像局部相关性的原理，对图像进行子抽样，可以减少数据处理量同时保留有用信息。通过去掉Feature Map中不重要的样本，进一步减少参数数量。

CNN输出图片尺寸
o=(i+2p-k)/s

CNN和传统的全连接神经网络有什么区别？

参数共享，连接稀疏


为什么参数稀疏会好？有什么数学原理么？

规则化符合奥卡姆剃刀(Occam's razor)原理。不过它的思想很平易近人：在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。


NMS(Non Maximum Suppression)，又名非极大值抑制，是目标检测框架中的后处理模块，主要用于删除高度冗余的bbox

手写nms

```python
#coding=utf-8
import numpy as np
def py_cpu_nms(dets, thresh):
    """Pure Python NMS baseline."""
    # tl_x,tl_y,br_x,br_y及score
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]

    #计算每个检测框的面积，并对目标检测得分进行降序排序
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]

    keep = []   #保留框的结果集合
    while order.size > 0:
        i = order[0]
        keep.append(i)　　#保留该类剩余box中得分最高的一个
        # 计算最高得分矩形框与剩余矩形框的相交区域
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

       #计算相交的面积,不重叠时面积为0
        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        
        #计算IoU：重叠面积 /（面积1+面积2-重叠面积）
        ovr = inter / (areas[i] + areas[order[1:]] - inter)

        #保留IoU小于阈值的box
        inds = np.where(ovr <= thresh)[0]
        order = order[inds + 1]   #注意这里索引加了1,因为ovr数组的长度比order数组的长度少一个

    return keep
    
 if __name__ == '__main__':
    dets = np.array([[100,120,170,200,0.98],
                     [20,40,80,90,0.99],
                     [20,38,82,88,0.96],
                     [200,380,282,488,0.9],
                     [19,38,75,91, 0.8]])

    py_cpu_nms(dets, 0.5)
```

高斯滤波：加权平均，高斯核（线性平滑滤波）：消除噪声
中值滤波：区域中值代替（非线性平滑）：消除噪声
均值滤波：区域中去掉自己的值求平均（线性滤波）：消除噪声


canny算子：高斯平滑->一阶偏导求梯度幅值和方向，非极大值抑制，根据阈值链接边缘
sobel算子：两个模板，与之卷积得到横向和纵向的亮度差分，一阶导数
Laplace算子：各向同性，二阶微分，灰度差值做比较



霍夫变换检测圆的原理

利用图像全局特征将边缘像素连接起来组成区域封闭边界，它将图像空间转换到参数空间，在参数空间对点进行描述，达到检测图像边缘的目的。

该方法把所有可能落在边缘上的点进行统计计算，根据对数据的统计结果确定属于边缘的程度。Hough 变换的实质就是对图像进行坐标变换，把平面坐标变换为参数坐标，使变换的结果更易识别和检测。


在模型效果不好的前提下，如何区分是过拟合还是模型复杂度不够



过拟合:训练拟合得很好，验证效果很差
解决方案：
1. 获取更多的数据
2. 使用合适的模型
    减少网络层数
    训练时间
    正则化（限制权值）
    增加噪音
3. 组合多种模型
    bagging(分段函数类似)
    boosting（多个简单网络，加权平均）
4. dropout
5. 贝叶斯



正则化（Regularization）是机器学习中一种常用的技术，其主要目的是控制模型复杂度，减小过拟合。最基本的正则化方法是在原目标（代价）函数 中添加惩罚项，对复杂度高的模型进行“惩罚”。
l0求非零元的个数
l1求绝对值（特征选择器）
l2求平方


l2正则化的效果是对原最优解的每个元素进行不同比例的放缩； l1正则化则会使原最优解的元素产生不同量的偏移，并使某些元素为0，从而产生稀疏性。

l2用于特征多重共线性问题


为什么不用L0(因为是一个np难问题)


L1为什么不用于卷积神经网络
死的神经元


梯度弥撒如何产生的，如何解决

产生：链式求导
解决：

- 预训练加微调
- 梯度剪切、权重正则（针对梯度爆炸）
- 使用不同的激活函数
- 使用batchnorm
- 使用残差结构
- 使用LSTM网络

batchnorm


sigmoid和relu对比；

relu 好处 relu后面为0是为了什么（面试官说是为了解决梯度发散问题，小于0时梯度为负，而且可能负得越来越大），这个问题是这样解释吗？如果relu的左半轴为负那么它的梯度不是还是定值吗只是为负了....
1. 防止梯度弥散
2. 稀疏激活性
3. 加快计算

sigmoid的导数只有在0附近的时候有比较好的激活性，在正负饱和区的梯度都接近于0，所以这会造成梯度弥散，

relu函数在大于0的部分梯度为常数，所以不会产生梯度弥散现象。

第二，relu函数在负半区的导数为0 ，所以一旦神经元激活值进入负半区，那么梯度就会为0，也就是说这个神经元不会经历训练，即所谓的稀疏性。

第三，relu函数的导数计算更快，程序实现就是一个if-else语句，而sigmoid函数要进行浮点四则运算。综上，relu是一个非常优秀的激活函数，鄙人的最爱


dropout 是指在深度网络的训练中, 以一定的概率随机地 “临时丢弃” 一部分神经元节点. 具体来讲, dropout 作用于每份小批量训练数据, 由于其随机丢弃部分神经元的机制, 相当于每次迭代都在训练不同结构的神经网络.

dropout前向和反向的处理；
前向：以p概率丢弃一个节点，通过的值放大1/（1-p)倍  保证总体信号不变
后向：前向得到d,误差为e de/dx=de/dd/dd/dx
dd/dx=概率函数

Loss不降等常见问题的处理方法：

1. 数据不对
2. 数据集打乱
3. 更改学习率
4. 网络没选对
5. batchsize过大（过小会导致模型摇摆不定）
6. 没有归一化
7. 模型瓶颈
8. 训练时间不够
9. 优化器不对
10. 合适的激活函数
11. 正则化过度（欠拟合）
12. 权重初始化有问题



熵：混乱程度（越高越混乱）

交叉熵怎么实现(如何计算)交叉熵损失函数定义，举例计算过程


分类问题用 cross entropy，回归问题用 mean squared error。

样本分布不平衡时，模型效果为什么不好？说明理由

focalloss

二分类效果是否好于多分类

二分类如何执行

逻辑回归

难例如何进行区分

推导BP神经网络




参数优化方法说一下(梯度下降的三种方式的优缺点)

深度学习里面的优化方法momentum和Adam来分别讲一下原理和公式

手写adam更新公式；


>稀疏数据如何建模/训练/优化
>流形数据的随机抽样



LR


极大似然估计

已知数据，推模型和参数。

极大似然估计和最大后验估计的区别是什么？

最大似然估计是求参数θ, 使似然函数P(x0|θ)最大。最大后验概率估计则是想求θ使P(x0|θ)P(θ)最大。（先验概率）

EM

svm:对偶问题，拉格朗日，优缺点等

Kmeans等常见机器学习算法的原理和推导



特征相关性指标(皮尔逊系数)
混淆矩阵
1. F1值的计算公式说一下？




讲一下unet和deeplabv2的流程，顺便问了下deeplabv3，

Xception网络含义

特征的提取和传递可以通过1*1卷积，3*3卷积，5*5卷积，pooling等，到底哪种才是最好的提取特征方式呢？Inception结构将这个疑问留给网络自己训练，也就是将一个输入同时输给这几种提取特征方式，然后做concat。



ResNet、DenseNet含义，处理方式，有什么好处，具体concat还是逐像素相加

在网络结构的设计上，经常说DenseNet和Inception中更多采用的是concatenate操作，而ResNet更多采用的add操作，那么这两个操作有什么异同呢？

Resnet是做值的叠加，通道数是不变的，DenseNet是做通道的合并。你可以这么理解，add是描述图像的特征下的信息量增多了，但是描述图像的维度本身并没有增加，只是每一维下的信息量在增加，这显然是对最终的图像的分类是有益的。而concatenate是通道数的合并，也就是说描述图像本身的特征增加了，而每一特征下的信息是没有增加。


神经网络分类的softmax数学公式，如何计算
exi/Eexi

普通卷积运算
输入   卷积核                      输出
3      3*3*4                       4
深度可分离卷积
3     3*3*3（组卷积，这里3组）      3
3     1*1*4                        4

小型网络有哪些

Mobilenet
ShuffNet

神经网络如何加速

1. 模型压缩
2. 紧凑网络
3. 权值分解
4. 剪枝
5. 二值化





# 算法


dp和递归的区别

重复的子问题，DP算法将其结果保存下来，等下一次又要计算该子问题时，直接调用已计算好的
而递归却不是这样，它会一遍又一遍地计算这些重复的子问题，从而效率狂降

正则表达式匹配

快排

二叉树前序遍历，

给一个二叉树和一个节点，找出该节点二叉树中序遍历下的下一个节点，如果树有父节点属性，空间复杂度0（1）找出来

pow(x,n)

链表的中间节点

链表反转

最长公共子序列

写一个字符串反转

判断链表是否有环

最长上升子序列


上台阶问题


字符串拼接成最大数字的排序


数组中最大子数组的和

矩阵中最大子矩阵块的和

分段二分查找

堆排序

旋转数组找K

二叉树镜像

判断平衡二叉树


无序数组中找K大的数

3. 你的解法详细说一下，时间复杂度是多少？

4. 为什么是O(n)，而不是O(nlogk)，来推导并且证明一下你的解法的时间复杂度(级数求和)

5. O(nlogk)的解法是怎么做的，你说一下(堆排序)

6. 现在把题目变一下：有4亿个数据，内存只能存1亿个数，找出第8000万大的数（楼主之前没了解过海量数据处理的问题，这一块崩了）

7. 这题可以用哈希吗？在什么情况下可以用哈希？

8. 你用堆排序做是吧？可以，堆排序这种方法有什么缺点？如果我要找第1.3亿大的数呢？


海量数据topk问题


设计一个结构来进行推荐的热词存储？1000w量级的（树结构）

输入con，就把con开头的热词都输出出来，设计数据结构，实现搜寻代码

蓄水池抽样算法



