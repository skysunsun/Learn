
# [BASNet: Boundary-Aware Salient Object Detection](http://openaccess.thecvf.com/content_CVPR_2019/papers/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.pdf)

## 研究问题

1. 显著性检测
2. 边界感知损失

## 显著性检测方法


## 网络结构

### U-Net部分

![niF1at.png](https://s2.ax1x.com/2019/09/02/niF1at.png)
基本操作，U-Net结构加上residual模块（作者这里改变了ResNet第一层的卷积核为$3\times3$,而且没有缩小特征尺寸，但是为了和ResNet有相同的感受野，作者在最后加了两个residual块（感觉得不偿失啊！前面不加，加在后面，严重加重计算量呀）），之后使用使用$3\times3$膨胀率为2的卷积到Decoder部分，Decoder（卷积加上采样）和Encoder（卷积加下采样）是对称的，而且每一个Decoder之后接一个$3\times3$的卷积然后上采样到和初始输入一样的大小
显著性图（侧输出：文中说这样是为了防止网络过拟合。我试过这种监督，训练非常慢）最后一个Decoder产生的显著性图输入到网络的下一部分

### 细化部分

这部分基本架构还是residual，作者在这里做了三个对比实验证明所提出的方法的有效性
![niu6Wn.png](https://s2.ax1x.com/2019/09/02/niu6Wn.png)
从粗略的显著性图到真值的细化，文中提到了迭代，金字塔，膨胀卷积等操作（作者说这些操作没有捕获到高语义信息），文中还是遵从U-Net的结构，但是更简单了，每层只有一个卷积层了（卷积核为$3\times3\times64$）

## 损失函数

文中用了BCE+SSIM+IOU
![niQYW9.png](https://s2.ax1x.com/2019/09/02/niQYW9.png)
这个图是横着看的，但是还是没看懂，而作者如是说：

>BCE损失是像素级的。它不考虑邻域的标签的影响，它对前景和背景像素的权重相等。这有助于在所有像素上的收敛。

>SSIM损失是块级的，它考虑每个像素的局部邻域。它赋予边界更高的权重，即使在边界和背景预测概率相同的情况下，边界附近的损失也会更高。在训练开始时，沿边界的损失最大(见图5第二行)，将优化的重点放在边界上。随着训练的进行，前景的SSIM损失减小，背景损失成为主导项。然而，当背景像素的预测非常接近真值时，背景损失才会对训练产生影响，此时背景像素的损失会迅速从1下降到0。这是有帮助的，因为预测通常在训练过程的后期接近于零，其中BCE损失变得平坦。SSIM损失确保仍有足够的梯度来推动学习过程。 由于概率被推到零，因此背景预测看起来更清晰。

但是SSIM评价的是图像的整体相似性（即结构相似性），因为他算的是整张图像的均值和标准差，但作者在这说SSIM对边界的损失贡献很大，上面这个图硬是没看懂，而且作者说，这个背景损失贡献了后期的梯度，损失函数不是只有一个么？bce小了，ssim不是也小了，背景贡献啥呀？难道一个损失函数包含了背景和前景两个损失？这不是打架么？

>IOU是地图级别的度量。随着对前景网络预测的置信度的提高，前景的损失最终降为零。当结合这三个损失时，我们使用BCE对所有像素保持平滑的渐变，同时使用IoU对前景给予更多的关注。SSIM是用来鼓励预测尊重原始图像的结构，通过更大的损失附近的边界。

这篇文章让我对线性插值有了新的认识，之前一直觉得线性插值不可学习，而且对于小目标不能还原回去，但是作者在线性插值中不停的穿插卷积，是这个弥补了线性插值的缺陷么？

而且这个网络两个模块都是下采样然后上采样，以前一直觉得这样不是信息丢失更多么？但是从这个网络的实验结果来看，效果还挺好！
