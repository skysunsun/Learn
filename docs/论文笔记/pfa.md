
# [Pyramid Feature Attention Network for Saliency detection](https://arxiv.org/pdf/1903.00179.pdf)（金字塔特征注意网络用于显著性检测）

## 针对问题

1. 如何提取有效的特征是显著性检测的一个关键点。
2. 目前的方法主要是不加区别地综合多尺度卷积特征。然而，并不是所有的特征都对显著性检测有用，有些甚至会引起干扰。
3. FCN池化层减少了特征映射的大小，并恶化了突出对象的边界。

## 模型结构

由于注意机制具有很强的特征选择能力，非常适合于显著性检测。不同的特征具有不同的语义值来生成显著性映射。
模型包含一个上下文感知金字塔特征提取模块和一个智能通道注意模块，以捕获上下文感知多尺度多感受域高级特征，空间注意模块用于低级特征映射细化显著对象细节和有效边缘保留损失，以指导网络在边界定位中学习更详细的信息。
[![Ek4rgH.png](https://s2.ax1x.com/2019/04/22/Ek4rgH.png)](https://imgchr.com/i/Ek4rgH)

基于视觉语境对显著性检测的重要性。

### SIFT

尺度不变特征变换(SIFT)是计算机视觉中用于检测和描述图像局部特征的一种特征检测算法。该算法提出了一种融合尺度空间表示和金字塔多分辨率表示的高斯表示法的拉普拉斯变换。用几种不同的高斯核函数处理具有相同分辨率的尺度空间表示;以及采用不同分辨率下采样处理的金字塔多分辨率表示。
*在SIFT特征提取的基础上，设计了一种新的提取尺度、形状和位置不变性特征的模块。*

### 高层特征

对于高层特征，采用上下文感知金字塔特征提取(CPFE)模块和智能信道感知注意模块来获取丰富的上下文信息。
在CPFE模块中，为了使最终提取的高级特征包含尺度和形状不变性的特征，在VGG网络的三个高层块上采用不同膨胀率的卷积（模仿不同核函数的高斯函数），分别设置为3、5和7来捕获多接收域上下文信息。然后将不同膨胀卷积层的特征映射和 $1\times1$ 降维特征通过跨通道级联进行组合。在此之后，获得了具有上下文感知信息的三种不同比例特征，我们将两个较小的特征上采样到最大的一个。 最后，通过跨渠道连接将它们组合为上下文感知金字塔特征提取模块的输出。
[![Ek4tD1.png](https://s2.ax1x.com/2019/04/22/Ek4tD1.png)](https://imgchr.com/i/Ek4tD1)
然后采用通道的注意机制，将较大的权值分配给对突出目标响应较强的信道。

### 低层特征

对于低层特征，存在一些背景区域分散了显著性映射的生成。空间注意机制根据高层次特征过滤掉一些背景细节，更多地关注前景区域，有助于生成有效的显著性预测特征。

### 通道注意机制

在上下文感知金字塔特征提取之后将通道智能注意（CA）模块添加到加权多尺度多感知域高级特征。 CA将为通道提供更大的权重，这些通道对显着对象表现出高响应。
$f^{h} \in \mathbb{R}^{W \times H \times C}$

$f^{h}=[f_{1}^{h}, f_{2}^{h}, \ldots, f_{C}^{h} ]$

$f_{i}^{h} \in \mathbb{R}^{W \times H}$ 为 $f^h$ 的第 $i$ 个切片，$C$ 为总通道数。

对每个 $f_{i}^{h}$ 应用平均池化，得到一个信道方向的特征向量 $v^h\in\mathbb{R}^{C}$。之后，两个连续的全连接(FC)层完全捕获信道依赖关系。为了限制模型的复杂性和辅助泛化，对信道方向的特征向量进行编码，在非线性周围形成两个FC层的瓶颈。然后，通过使用sigmoid操作，对编码后映射到 $[0,1]$ 的信道方向特征向量进行归一化处理。

$C A=F(v^{h}, W)=\sigma_{1}(f c_{2}(\delta(f c_{1}(v^{h}, W_{1})), W_{2}))$

#### 参数说明

$W$ 为通道智能注意块参数

$\sigma_{1}$ 为sigmoid操作

$f c$为fc层

$\delta$ 为ReLU函数

块的最终输出 $\widetilde{f}^{h}$ 是通过使用CA对上下文感知的金字塔特征进行加权得到的。

$\widetilde{f}^{h}=C A \cdot f^{h}$

### 空间注意

自然图像通常包含丰富的前景细节和复杂的背景。低层特征的显著性图包含了很多细节，容易导致不好的结果。在显著性检测中，希望在没有其他纹理干扰的情况下，获得突出物体与背景之间的详细边界。因此，不将所有的空间位置都考虑在内，而是采用空间注意的方法，将更多的注意力集中在前景区域，这有助于生成有效的显著性预测特征。

底层特性表示为 $f^{l} \in \mathbb{R}^{W \times H \times C}$

空间位置集合用 $\mathbb{R}=\{(x, y) | x=1, \ldots, W ; y=1, \dots, H \}$

 $j=(x,y)$ 为低层特征的空间坐标。

 为了增加接收域，获取全局信息，但不增加参数，使用了两个卷积层，一个核为 $1 \times \mathrm{k}$ ，另一个核为 $\mathrm{k} \times 1$ ，用于高层特征捕获空间关注点。然后，利用sigmoid操作，对映射到 $[0,1]$ 的编码空间特征图进行归一化处理。

$C_{1}=\operatorname{con} v_{2}(\operatorname{con} v_{1}(\widetilde{f}^{h}, W_{1}^{1}), W_{1}^{2}) )$

$C_{2}=\operatorname{con} v_{1}(\operatorname{con} v_{2}(\widehat{f}^{h}, W_{2}^{1}), W_{2}^{2}) )$

$S A=F(\widehat{f}^{h}, W)=\sigma_{2}(C_{1}+C_{2})$

#### 参数说明

$W$ 为空间注意块参数

$\sigma_{2}$ 为sigmoid操作

$\operatorname{conv}_{1}$ 和 $\operatorname{conv}_{2}$ 分别为 $1 \times \mathrm{k} \times \mathrm{C}$ 和 $\mathrm{k} \times 1 \times 1$ 卷积层

$\delta$ 为ReLU函数

块的最终输出 $\widetilde{f}^{l}$ 用SA加权得到。
$\widetilde{f}^{l}=S A \cdot f^{l}$

[![Ek4dUK.png](https://s2.ax1x.com/2019/04/22/Ek4dUK.png)](https://imgchr.com/i/Ek4dUK)

## 损失函数

### 交叉熵损失函数

$L_{S}=-\sum_{i=0}^{s i z e(Y)}(\alpha_{s} Y_{i} \log (P_{i})+(1-\alpha_{s})(1-Y_{i}) \log (1-P_{i}) )$

#### 参数说明

$Y$ 为真值

$P$ 为网络输出的显著图

$\alpha_{s}$ 为正负样本平衡参数

损失函数仅提供一般指导产生显著图。使用一个更简单的策略来强调突出对象边界细节的生成。首先利用拉普拉斯算子得到真值和网络显著性输出图的边界，然后利用交叉熵损失来监督突出目标边界的生成。

拉普拉斯算子是n维欧氏空间中的二阶微分算子，定义为梯度的散度($\Delta f$)，由于二阶导数可以用来检测边缘，所以使用拉普拉斯算子来得到显著的物体边界。

#### 二维拉普拉斯算子

$\Delta f=\frac{\partial^{2} f}{\partial x^{2}}+\frac{\partial^{2} f}{\partial y^{2}}$

其中x和y为标准xy平面的笛卡尔坐标。事实上，由于拉普拉斯算子使用图像的梯度，它在内部调用卷积运算来执行计算。然后使用tanh激活函数将值映射到 $[0,1]$ ，最后使用绝对值操作

$\Delta \widetilde{f}=a b s(\tanh (\operatorname{conv}(f, K_{\text {laplace}})))$

最后利用交叉熵损失来监督目标边界的生成

$L_{B}=-\sum_{i=0}^{\operatorname{size}(Y)}(\Delta Y_{i} \log (\Delta P_{i})+(1-\Delta Y_{i}) \log (1-\Delta P_{i}) )$

### 最终损失函数

总损失函数为它们的加权和:

$L=\alpha L_{S}+(1-\alpha) L_{B}$

## 思考
### 好的地方
1. 在特征层的提取上运用了注意机制
2. 在高低层次的特征运用不同的注意机制
3. 拉普拉斯用于边缘细节的细化


### 没看懂的地方
1. 文中说到，高层特征和低层特征用的是不一样的注意机制，但是根据后面的公式，空间注意机制的权值是根据高层特征的输出来生成的？？？？
2. 关于通道注意机制的算法。和空间注意机制算法（这些算法为什么有效呢？）
