
# [Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

## 针对问题

1. 深度模型的难以训练
2. 随着网络深度的增加，梯度消失/爆炸
3. 深度增加，精度饱和之后开始退化

## 深度残差学习

### 残差学习
几个堆叠的层(不一定是整个网络)形成的映射$\mathcal{H}(\mathbf{x})$，$x$表示这些层中第一个层的输入。假设多个非线性层可以渐进逼近复杂函数，则等价于假设它们可以渐进逼近残差函数，即$\mathcal{H}(\mathbf{x})-\mathbf{x}$。因此，我们不期望堆叠层近似于$\mathcal{H}(\mathbf{x})$，而是显式地让这些层近似于残差函数$\mathcal{F}(\mathbf{x}) :=\mathcal{H}(\mathbf{x})-\mathbf{x}$。原来的函数变成$\mathcal{F}(\mathbf{x})+\mathbf{x}$。虽然这两种形式都应该能够渐进地逼近所需的函数(假设的那样)，但学习的容易程度可能有所不同。

如果可以将添加的层构造为恒等映射，那么较深的模型应该不会比较浅的模型有更大的训练误差。退化问题表明，求解者可能难以用多个非线性层逼近恒等映射。在重构的残差学习公式中，如果恒等映射是最优的，求解者可以简单地将多个非线性层的权值趋近于零来逼近恒等映射。

在实际情况中，恒等映射不太可能是最优的，但是我们的重新构造可能有助于预先解决问题。如果最优函数更接近于恒等映射而不是零映射，那么求解者应该更容易找到与恒等映射相关的扰动，而不是将函数作为一个新的函数来学习。实验表明，学习后的残差函数一般响应较小，这说明恒等映射提供了合理的预处理。

### 通过快捷方式进行恒等映射
![ZU61oj.png](https://s2.ax1x.com/2019/07/04/ZU61oj.png)
$$
\mathbf{y}=\mathcal{F}\left(\mathbf{x},\left\{W_{i}\right\}\right)+\mathbf{x}
$$
因为是逐元素相加，所以$\mathcal{F}$和$X$的维度必须相等，如果不等可以通过下面的公式调节
$$
\mathbf{y}=\mathcal{F}\left(\mathbf{x},\left\{W_{i}\right\}\right)+W_{s} \mathbf{x}
$$
还有一种方法就是补零

### 网络结构

卷积层大多是$3\times3$卷积核，并遵循两个简单的设计规则:

1. 对于相同的输出特征图大小，各层具有相同数量的卷积核
2. 如果将特征图的大小减半，则会将卷积核的数量增加一倍，以保持每一层的时间复杂度

直接通过步长为2的卷积层执行下行采样。网络以一个全局平均池层和一个带有$softmax$的1000个节点的全连接层结束。
![ZUgaa4.png](https://s2.ax1x.com/2019/07/04/ZUgaa4.png)

![ZURStH.png](https://s2.ax1x.com/2019/07/04/ZURStH.png)

果然强大的东西都是比较简单的。大道至简呀！这部分就一个东西，残差学习可能比0值学习更加易于求解

## 问题

1. 网络中一般是四个块，每个块包含多少层这个系数到底是怎么来的呀？实验做出来的么？

是模仿VGG-16么？为什么是第三块最多呢?
