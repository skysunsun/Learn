# [A2-Nets: Double Attention Networks]()


## 问题


### 为了捕获长距离的关系，CNN一般通过加深网络来捕获特征长距离的相关性，但是这么做并不高效

- 模型大不仅增加了计算成本，同时还有过拟合的风险
- 层太多，优化困难(梯度消失问题？)
- 层太多有延迟？
- 当前一些方法并不是很灵活

## 方法

针对上面的问题，作者提出了一种双注意力的模块，一个收集全局信息，一个分发全信息。

[![DgN9fS.png](https://s3.ax1x.com/2020/11/29/DgN9fS.png)](https://imgchr.com/i/DgN9fS)


- 全局信息收集模块

感觉作者说了很大一堆，然后就是一个$softmax$

$$G= A \times softmax(B)$$

其中$A,B$为两个特征(可以是相同卷积得到的特征)


- 信息分发模块

然后作者用了相同的操作？？？一个操作用两次，然后这也算双注意力？？？

$$
Z= G \times softmax(V)
$$

其中$V$也是由卷积得到的特征

？？？这好像没有区别呀。作者说这有特征选择的作用？？？

看到这里感觉就和NLP的注意力机制有点类似了，但是一旦用到$softmax$运算就不会很快。


而且这种东西一般是用在网络的最后一层，作者说放前面没有放后面效果好(哈哈哈哈)。这种东西一旦放到网络的前面一般就会显存爆炸。。。。



而感觉这篇论文对于长距离的关系捕获就是一个$softmax$。
